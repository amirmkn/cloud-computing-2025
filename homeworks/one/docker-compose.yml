services:
  llm-inference-service:
    image: amirmkn/llm-inference-image # Use your own image (that already pushed to docker hub)
    ports:
      - "8080:5000" # Host port 8080 maps to container port 5000
    environment:
      METRICS_LOG_FILE : inside_compose_inference_metrics.csv # Set the `METRICS_LOG_FILE` value to be `inside_compose_inference_metrics.csv`
    volumes:
      - ./compose_inference_metrics.csv:/app/inside_compose_inference_metrics.csv # Mount volume to get output in you host machine in a file named `compose_inference_metrics.csv`